# ðŸ§  RAG Cortex

A powerful Retrieval-Augmented Generation (RAG) chatbot that lets you chat with your PDF documents. Built with **Streamlit**, **LangChain**, **Groq**, and **ChromaDB**.

## âœ¨ Features
- **Chat with PDFs** â€” Ask questions and get accurate answers based on your documents
- **Dark Notion-style UI** â€” Clean, minimal dark theme interface
- **Document Management** â€” Add and delete documents from the sidebar
- **Duplicate Detection** â€” Warns before re-uploading files already indexed
- **Semantic Chunking** â€” Splits by topic, not arbitrary character counts
- **Text Preprocessing** â€” Removes citations, page numbers, and bibliography noise
- **Cross-Encoder Reranking** â€” Filters irrelevant results using semantic relevance scoring
- **Persistent Memory** â€” ChromaDB saves embeddings to disk (load in seconds)
- **High-Performance LLM** â€” Groq API running Llama 3.3 70B
- **Local Embeddings** â€” Ollama `nomic-embed-text` for private processing

## ðŸ› ï¸ Tech Stack
| Component | Tool | Why? |
|-----------|------|------|
| **Frontend** | Streamlit | Fast, interactive UI in pure Python |
| **Framework** | LangChain | Orchestrates the RAG pipeline |
| **LLM** | Groq API | Extremely fast inference for Llama 3 |
| **Embeddings** | Ollama | Runs `nomic-embed-text` locally |
| **Vector Store** | ChromaDB | Persists to disk (unlike RAM-only FAISS) |
| **PDF Parser** | PyMuPDF | Better text extraction than PyPDF |
| **Reranker** | Cross-Encoder | Filters irrelevant results with semantic scoring |

## ðŸ“ Project Structure
```
RAG-Chatbot/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.py          # Streamlit UI
â”‚   â”œâ”€â”€ rag_core.py     # RAG logic
â”‚   â””â”€â”€ styles.css      # Dark theme CSS
â”œâ”€â”€ documents/          # Your PDFs go here
â”œâ”€â”€ rag_vector_store/   # ChromaDB persistence
â””â”€â”€ .env                # API keys
```

## âš™ï¸ Setup

1. **Prerequisites**:
   - Python 3.13+
   - [Ollama](https://ollama.com/) installed
   - Pull the embedding model: `ollama pull nomic-embed-text`

2. **Install Dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Environment Variables**:
   Create a `.env` file in the root directory:
   ```env
   GROQ_API_KEY=your_groq_api_key_here
   ```

4. **Run the App**:
   ```bash
   cd src
   streamlit run app.py
   ```

## ðŸ§  How It Works
1. **Ingestion** â€” Scans `documents/` folder for PDFs (PyMuPDF)
2. **Cleaning** â€” Removes citations, page numbers, bibliography entries
3. **Chunking** â€” Semantic splitting by topic shifts, with size limits
4. **Embedding** â€” Converts text to vectors via `nomic-embed-text`
5. **Storage** â€” Saves vectors to `rag_vector_store/` (ChromaDB)
6. **Retrieval** â€” Fetches top 20 similar chunks for your question
7. **Reranking** â€” Cross-Encoder scores relevance, filters to top 3
8. **Generation** â€” Sends question + context to Groq (Llama 3.3)

## ðŸ’¡ Lessons Learned

### Vector Store: FAISS vs Chroma
- **FAISS**: Stores in RAM, requires re-processing on restart
- **Chroma**: Persists to disk, instant 2-second reload âœ“

### Embeddings: Speed vs Accuracy
- **HuggingFace** (`all-MiniLM-L6-v2`): Fast but lower accuracy
- **Ollama** (`nomic-embed-text`): Best balance, 8192 token context âœ“
- **FastEmbed** (`BAAI/bge-small`): Future option for 1000+ docs

### Chunking: Character vs Semantic
| Method | How it works | Pros | Cons |
|--------|--------------|------|------|
| **Character** | Cut every N chars | Simple, fast | Breaks mid-sentence |
| **Semantic** | Split by topic shifts | Coherent chunks | Variable sizes |
| **Recursive Semantic** | Semantic + size limits | Best of both âœ“ | More complex |
| **Small-to-Big** | Search small chunks, return parent context | Very precise search + full context | Complex metadata linking |


### PDF Parsing: PyPDF vs PyMuPDF
- **PyPDF**: Simple but breaks text with unusual fonts (`"Ar e W e"`)
- **PyMuPDF**: Handles styled text, fonts, and formatting better âœ“

### Text Preprocessing Trade-offs
Regex cleaning removes citations and page numbers but may catch valid content like "Table 1". The **reranker** post-retrieval filters irrelevant results more intelligently using semantic understanding.

### Reranking: Why Cross-Encoder?

| Reranker | Latency (20 docs) | MRR@10* | Cost | Complexity |
|----------|-------------------|---------|------|------------|
| **Cross-Encoder** | ~150ms | 0.39 | Free (local) | Low âœ“ |
| **ColBERT** | ~50ms | 0.36 | Free (local) | High (GPU) |
| **LLM-as-Reranker** | ~2s | 0.40+ | API costs | Low |
| **Cohere API** | ~100ms | 0.40 | Per-request | Very Low |

*MRR@10 = Mean Reciprocal Rank on MS MARCO passage reranking benchmark

**Why Cross-Encoder:** Best local accuracy (MRR 0.39), runs locally with no API costs, simple integration with `sentence-transformers`, and works well for small candidate sets (k â‰¤ 25).

### Reranker Implementation: `.rank()` vs `.predict()`
| Method | Pros | Cons |
|--------|------|------|
| `.predict()` | Full control | Manual sorting required |
| `.rank()` | Built-in sorting, cleaner API âœ“ | Less flexible |

Using `Sigmoid()` activation converts raw logits to 0-1 probability scores for interpretability.

### Rejected Idea: Passing Relevance Scores to LLM
**Idea:** Include relevance scores with context so LLM can weight sources differently.

**Why rejected:**
- LLMs don't reason well about numerical scores
- Document ordering already conveys importance
- Risk of LLM ignoring correct content due to low score
- Adds prompt complexity without clear benefit

### Known Limitations: Styled Text Extraction
PyMuPDF sometimes fails to extract styled text (bold, italic, hyperlinks, colored text).

**Examples encountered:**
- `"Cyberpunk"` (blue hyperlink) â†’ extracted as blank
- `"MMLU"` (bold italic) â†’ not captured

**Workaround:** The reranker's `min_score=0.3` threshold filters out irrelevant results.

**Future solution:** Multimodal RAG using vision models to "see" PDFs as images.

---

## ðŸ—ï¸ Architecture

```mermaid
flowchart TB
    subgraph INGESTION ["ðŸ“¥ Document Ingestion"]
        A[("ðŸ“„ Raw PDFs")] --> B["PyMuPDF Loader"]
        B --> C["Text Extraction"]
        C --> D["clean_document_text()"]
        D --> E["Remove citations, page numbers, bibliography"]
    end

    subgraph CHUNKING ["âœ‚ï¸ Semantic Chunking"]
        E --> F["SemanticChunker"]
        F --> G["Split by topic shifts"]
        G --> H["RecursiveCharacterTextSplitter"]
        H --> I["Enforce max size (1500 chars)"]
        I --> J["Merge tiny chunks (<200 chars)"]
    end

    subgraph EMBEDDING ["ðŸ”¢ Embedding & Storage"]
        J --> K["Ollama nomic-embed-text"]
        K --> L["Generate 768-dim vectors"]
        L --> M[("ChromaDB")]
    end

    subgraph RETRIEVAL ["ðŸ” Query Pipeline"]
        N["â“ User Question"] --> O["Embed Question"]
        O --> P["Vector Similarity Search (k=20)"]
        M --> P
        P --> Q["Top 20 Candidates"]
    end

    subgraph RERANKING ["ðŸŽ¯ Cross-Encoder Reranking"]
        Q --> R["ms-marco-MiniLM-L-6-v2"]
        R --> S["Score each (query, doc) pair"]
        S --> T["Filter min_score â‰¥ 0.3"]
        T --> U["Return Top 3"]
    end

    subgraph GENERATION ["ðŸ’¬ Response Generation"]
        U --> V["Build Context"]
        N --> W["Groq API (Llama 3.3 70B)"]
        V --> W
        W --> X["ðŸŽ¯ Final Answer"]
    end

    style INGESTION fill:#1a1a2e,stroke:#16213e
    style CHUNKING fill:#1a1a2e,stroke:#16213e
    style EMBEDDING fill:#1a1a2e,stroke:#16213e
    style RETRIEVAL fill:#0f3460,stroke:#16213e
    style RERANKING fill:#533483,stroke:#16213e
    style GENERATION fill:#e94560,stroke:#16213e
```
