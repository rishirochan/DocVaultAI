# ğŸ§  RAG Cortex

A powerful Retrieval-Augmented Generation (RAG) chatbot that lets you chat with your PDF documents. Built with **Streamlit**, **LangChain**, **Groq**, and **ChromaDB**.

## âœ¨ Features
- **Chat with PDFs** â€” Ask questions and get accurate answers based on your documents
- **Dark Notion-style UI** â€” Clean, minimal dark theme interface
- **Document Management** â€” Add and delete documents from the sidebar
- **Duplicate Detection** â€” Warns before re-uploading files already indexed
- **Semantic Chunking** â€” Splits by topic, not arbitrary character counts
- **Text Preprocessing** â€” Removes citations, page numbers, and bibliography noise
- **Persistent Memory** â€” ChromaDB saves embeddings to disk (load in seconds)
- **High-Performance LLM** â€” Groq API running Llama 3.3 70B
- **Local Embeddings** â€” Ollama `nomic-embed-text` for private processing

## ğŸ› ï¸ Tech Stack
| Component | Tool | Why? |
|-----------|------|------|
| **Frontend** | Streamlit | Fast, interactive UI in pure Python |
| **Framework** | LangChain | Orchestrates the RAG pipeline |
| **LLM** | Groq API | Extremely fast inference for Llama 3 |
| **Embeddings** | Ollama | Runs `nomic-embed-text` locally |
| **Vector Store** | ChromaDB | Persists to disk (unlike RAM-only FAISS) |
| **PDF Parser** | PyMuPDF | Better text extraction than PyPDF |

## ğŸ“ Project Structure
```
RAG-Chatbot/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.py          # Streamlit UI
â”‚   â”œâ”€â”€ rag_core.py     # RAG logic
â”‚   â””â”€â”€ styles.css      # Dark theme CSS
â”œâ”€â”€ documents/          # Your PDFs go here
â”œâ”€â”€ rag_vector_store/   # ChromaDB persistence
â””â”€â”€ .env                # API keys
```

## âš™ï¸ Setup

1. **Prerequisites**:
   - Python 3.13+
   - [Ollama](https://ollama.com/) installed
   - Pull the embedding model: `ollama pull nomic-embed-text`

2. **Install Dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Environment Variables**:
   Create a `.env` file in the root directory:
   ```env
   GROQ_API_KEY=your_groq_api_key_here
   ```

4. **Run the App**:
   ```bash
   cd src
   streamlit run app.py
   ```

## ğŸ§  How It Works
1. **Ingestion** â€” Scans `documents/` folder for PDFs (PyMuPDF)
2. **Cleaning** â€” Removes citations, page numbers, bibliography entries
3. **Chunking** â€” Semantic splitting by topic shifts, with size limits
4. **Embedding** â€” Converts text to vectors via `nomic-embed-text`
5. **Storage** â€” Saves vectors to `rag_vector_store/` (ChromaDB)
6. **Retrieval** â€” Finds most similar chunks for your question
7. **Generation** â€” Sends question + context to Groq (Llama 3.3)

## ğŸ’¡ Lessons Learned

### Vector Store: FAISS vs Chroma
- **FAISS**: Stores in RAM, requires re-processing on restart
- **Chroma**: Persists to disk, instant 2-second reload âœ“

### Embeddings: Speed vs Accuracy
- **HuggingFace** (`all-MiniLM-L6-v2`): Fast but lower accuracy
- **Ollama** (`nomic-embed-text`): Best balance, 8192 token context âœ“
- **FastEmbed** (`BAAI/bge-small`): Future option for 1000+ docs

### Chunking: Character vs Semantic
| Method | How it works | Pros | Cons |
|--------|--------------|------|------|
| **Character** | Cut every N chars | Simple, fast | Breaks mid-sentence |
| **Semantic** | Split by topic shifts | Coherent chunks | Variable sizes |
| **Recursive Semantic** | Semantic + size limits | Best of both âœ“ | More complex |
| **Small-to-Big** | Search small chunks, return parent context | Very precise search + full context | Complex metadata linking |


### PDF Parsing: PyPDF vs PyMuPDF
- **PyPDF**: Simple but breaks text with unusual fonts (`"Ar e W e"`)
- **PyMuPDF**: Handles styled text, fonts, and formatting better âœ“

### Text Preprocessing Trade-offs
Regex cleaning removes citations and page numbers but may catch valid content like "Table 1". A **reranker** post-retrieval is planned to filter irrelevant results more intelligently.
